# Self-play training

Configs for training using some kind of RL or self-play trainings. The code is in the `faridiplomacy/selfplay/exploit.py`. See `ExploitTask` in [conf.proto](../conf.proto) for details on flags.

Main training models:

  * Training a policy network to exploit a dipnet agent using policy gradient. Configs: `exploit_*`
  * Training a policy network via self-play using policy gradients. Configs: `selfplay_*`.
  * Training policy and/or value via Deep Nash Learning. Configs: `research_*`.



  ## Deep Nash Value Iteration Learning

  There 3 base configs for DNVI/DORA for agents that was used in DORA paper for 7p agents:

   * `research_20210501_paper_gunboat_scratch_stage{1,2}` are stages 1 and 2 to train DORA, i.e., a from-scratch agent with DO. The longer stage 2 trains, the getter it gets.
   * `research_20210501_paper_gunboat_human_npu` is the HumanDNVI-NPU config, i.e., networks were initialized from a supervised model and policy updates were not propogated to rollout workers. Deviates from old graphconv models after 500 epochs.

